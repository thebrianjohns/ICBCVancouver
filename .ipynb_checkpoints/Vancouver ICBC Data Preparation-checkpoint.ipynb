{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b0f555",
   "metadata": {},
   "source": [
    "# Cleaning ICBC Accident Data for Vancouver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b283af",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f296d940",
   "metadata": {},
   "source": [
    "In British Columbia, car insurance is provided by a single crown corporation: Insurance Corporation of British Columbia (ICBC).  As a result of being a crown corporation, they are beholden to open data policies within the province similar to a government agency.  As such, a significant amount of data about the accidents that occur in the province is made available and can be found [HERE](https://www.icbc.com/about-icbc/newsroom/Pages/Statistics.aspx)\n",
    "\n",
    "Much of the data can be broken down regionally in BC, or by municipality.  Being a Vancouver resident, I am aware that there are a number of unique neighborhoods and communities, 22 in all, within Vancouver.  Given the number of unique neighborhoods in Vancouver, I wanted to see if **there are any trends or insights that could be made about ICBC reported accidents at the neighborhood level in Vancouver.**\n",
    "\n",
    "The neighborhood information had to be added into the data that was available.  This required taking separate coordinate data from the city of Vancouver that provides the boundaries for each neighborhood.  This can be found [HERE](https://opendata.vancouver.ca/explore/dataset/local-area-boundary/export/?disjunctive.name).\n",
    "\n",
    "For this project, the primary analysis will be done in MySQL.  In preparation for that analysis, this is what will be accomplished in this notebook:\n",
    "\n",
    "1. Cleaning the Vancouver Neighborhood Coordinate data in order to make 2-D boundaries for each neighborhood\n",
    "2. Use the coordinates in the Accident Data to label each with the the neighborhood that it occured in\n",
    "3. Cleaning of the finalized Accident Data to remove duplicative or redundent data in preparation for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6502128",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Cleaning Vancouver Neighborhood Data](#neighborhood)\n",
    "2. [Labeling the Neighborhood of Each Accident](#labeling)\n",
    "3. [Preliminary Data Cleaning](#cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da785a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Polygon used to create shapes of the neighborhoods\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "#Point used to compare accident coordinates to the Polygon shape and label neighborhoods\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb43910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b814ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e7c87",
   "metadata": {},
   "source": [
    "## Cleaning Vancouver Neighborhood Data\n",
    "\n",
    "<a id = 'neighborhood'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806d7e22",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/local-area-boundary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/local-area-boundary.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/local-area-boundary.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/local-area-boundary.csv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061db72",
   "metadata": {},
   "source": [
    "When inputting the data in this fashion, it is later revealed that the 'Shaughnessy' neighborhood line of data was not able to be read through in this process.  This is fixed when the neighborhood labels are applied to the accident data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.set_index('level_0')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95724d20",
   "metadata": {},
   "source": [
    "Index is a bit of a mess, but the neighborhood names are inside of it.  I will primarily be using indexing and slicing in order to make the data appropriately readable for analysis.\n",
    "\n",
    "First I will deal with the index, split the pieces and place them in the appropriate place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180863e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each individual piece of data is separated by a ';'\n",
    "index_list = list(df.index.str.split(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b63fa",
   "metadata": {},
   "source": [
    "Each piece of the index has the following:\n",
    "1. Abbrevation of the neighborhood.  This will not be kept.\n",
    "2. Full neighborhood name.  This will become the new index for the data.\n",
    "3. A string that contains \"coordinates\" followed by the first coordinate of the location.  This will be cleaned so that just the coordinate remains and is placed as the first column of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a list of just the neighborhood names\n",
    "neighbourhood_list = []\n",
    "for n in range(0,len(index_list)):\n",
    "    neighbourhood = index_list[n][1]\n",
    "    neighbourhood_list.append(neighbourhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d596fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539853a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the appropriate place to index to get just the coordinate number\n",
    "index_list[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list[0][2][-21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f589361",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_list = []\n",
    "for n in range(0,len(index_list)):\n",
    "    coordinate = index_list[n][2][-21:]\n",
    "    coordinate_list.append(coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670057aa",
   "metadata": {},
   "source": [
    "Due to rounding, some coordinates have a different number of digits and '\\['.  Indexing will be used to clean this further and match the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38832e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying rows that don't match\n",
    "corrections = [2, 4, 5, 13, 18]\n",
    "\n",
    "#Reverse indexing these rows to get the same number of '[' at the front of the coordinate\n",
    "for n in corrections:\n",
    "    coordinate_list[n] = coordinate_list[n][-20:]\n",
    "    \n",
    "coordinate_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37828e2",
   "metadata": {},
   "source": [
    "With the neighborhood list and the coordinate list, we are ready to change the index and add the coordinates as the first column of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making index just the neighborhood names\n",
    "df.index = neighbourhood_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa303bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting coordinates into the first column of data\n",
    "idx = 0\n",
    "df.insert(loc=idx, column='level_0', value=coordinate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64167f",
   "metadata": {},
   "source": [
    "Unseen in the data above, there is actually a blank space in the front of every datapoint with the exception of 'level_0' as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0615ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['level_0'][0][:])\n",
    "print(df['level_0'][0][2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['level_2'][0][:])\n",
    "print(df['level_2'][0][2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbdd75",
   "metadata": {},
   "source": [
    "I have kept the second '\\[' in level_0 so that every data point can be accurately indexed together.  The longitudes will have the first two characters removed, the latitudes will have the last character removed, and this will result in just the number remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even rows are longitudes\n",
    "for n in range(0, 221, 2):\n",
    "    name = 'level_'+str(n)\n",
    "    df[name] = df[name].str[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Odd rows are latitudes\n",
    "for n in range(1, 222, 2):\n",
    "    name = 'level_'+str(n)\n",
    "    df[name] = df[name].str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82557b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c35dac",
   "metadata": {},
   "source": [
    "The last 3 columns still need some cleaning.  The third to last column still has ']]' for the final latitude.  The final two columns has what I believe to be the centroid location of the neighborhood shape.  This data will not be kept for this analysis and will be removed.\n",
    "\n",
    "To faciliate this, I will transpose the dataframe, make a list of the coordinates and modify/remove the last 3 items of the listed coordinates before re-transposing the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23732fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Dataframe will be made to add the modified lists to\n",
    "df2 = pd.DataFrame(columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3490147",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in df.columns:\n",
    "    #Creating list of coordinates for each neighborhood\n",
    "    listed = list(df[n])\n",
    "    length = len(listed)\n",
    "    \n",
    "    #With differing lengths of coordinates, index relative to the length of the specific neighborhood's list\n",
    "    for item in range(0, length-2):\n",
    "        \n",
    "        #First find the item with ']]' at the end\n",
    "        if str(listed[item]).endswith(']]'):\n",
    "            \n",
    "            #Remove the ']]'\n",
    "            listed[item] = listed[item][:-2]\n",
    "            \n",
    "            #Delete the two list items after it\n",
    "            del listed[item+2]\n",
    "            del listed[item+1]\n",
    "    \n",
    "    #Add the modified list to the new dataframe\n",
    "    df2[n] = listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fe04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose this dataframe and inspect for the change in the last 3 columns\n",
    "df2 = df2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e40a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11c0a6",
   "metadata": {},
   "source": [
    "Here we have cleaned up the coordinates, but there is still a few things needed to be prepared for making Polygons. To do so, I will do the following:\n",
    "\n",
    "1. To facilitate future calculations and transformations considering there are a different number of coordinates for each neighborhood, I will change all of the Nan values to 0's.\n",
    "2. I will create new columns that will include the X and Y coordinates as a tuple.\n",
    "3. Then I will keep only the columns that have the full coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing nan's with 0's, this will help ensure every future tuple is treated as a float.\n",
    "df2 = df2.replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93399e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring column names are strings so they can be edited with ease later.\n",
    "df2.columns = df2.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All datapoints are objects and now need to be converted to numeric values prior to creating tuples.\n",
    "for col in df2.columns:\n",
    "    try:\n",
    "        df2[col] = pd.to_numeric(df2[col])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880341fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming all data has been turned into floats\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the length of the columns\n",
    "column_length = len(df2.columns)\n",
    "\n",
    "#Counter to help appropriately name new columns\n",
    "count = 1\n",
    "\n",
    "for n in range(0, column_length, 2):\n",
    "    #Creating new column name\n",
    "    column_number = (f'Coordinate #'+str(count))\n",
    "    \n",
    "    #Collecting longitude\n",
    "    x_coor = df2.columns[n]\n",
    "    \n",
    "    #Collecting the matching latitude in following column\n",
    "    y_coor = df2.columns[n+1]\n",
    "    \n",
    "    #Creating new column with the coordinates zipped together as a tuple\n",
    "    df2[column_number] = list(zip(df2[x_coor], df2[y_coor]))\n",
    "    \n",
    "    #Increase count to rename the next column\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ac56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.iloc[:,222:].T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b64d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New dataframe indexing just the tupled coordinates\n",
    "coor_df = df2.iloc[:,222:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e762d5",
   "metadata": {},
   "source": [
    "We now have a cleaned dataset where every column is the neighborhood and every row is the full longitude/latitude coordinate that is part of the border for each neighborhood.\n",
    "\n",
    "With this we can now make Polygons for each neighborhood.  First I would like to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_list = coor_df['Dunbar-Southlands']\n",
    "dunbar = Polygon(coord_list)\n",
    "dunbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dunbar is the only neighborhood that does not have any (0,0) coordinates from previous Nan values\n",
    "#I'll test again with a different neighborhood, only selecting relevant coordinates\n",
    "fairview_coor = coor_df['Fairview'].loc[coor_df['Fairview'] != (0.0, 0.0)]\n",
    "fairview = Polygon(fairview_coor)\n",
    "fairview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f4ab7",
   "metadata": {},
   "source": [
    "Success!  The next step now will be to import the accident data and label each accident with the neighborhood that it occured in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f246fa",
   "metadata": {},
   "source": [
    "## Labeling the Neighborhood of Each Accident\n",
    "\n",
    "<a id = 'labeling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc693080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting through VS Code, this data has different encoding than the default for python\n",
    "accident_df = pd.read_csv('data/bc_full_data.csv', encoding = 'utf-16 le', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35baae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee894f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af22215",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df[accident_df['Latitude'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{round(accident_df[\"Latitude\"].isna().sum()/accident_df.shape[0]*100, 2)}% of the accidents have Null Location data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9239ede",
   "metadata": {},
   "source": [
    "An initial inspection shows that some cleaning will be needed prior to analysis.\n",
    "\n",
    "1. Numerous columns look to be redundent, but will need to be confirmed.\n",
    "\n",
    "2. Nearly 11% of the data does not have location information.  This is a sizeable chunk of the data, but there is no effective way to fill this data based on the information available at this time.  We will still have +200K accidents with locations, so the accidents with null values will be dropped.\n",
    "\n",
    "3. Some reorganization of the data will likely be helpful.  This will be done when the data is imported into SQL.\n",
    "\n",
    "I will drop the null values first, and then label each accident with the appropriate neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c27667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming that Latitude and Longitude are both null in the same rows\n",
    "accident_df[accident_df['Latitude'].isna() & accident_df['Longitude'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New dataframe including only values that are notna in Latitude and resetting the index\n",
    "accident_df = accident_df[accident_df['Latitude'].notna()]\n",
    "accident_df = accident_df.reset_index(drop = True)\n",
    "accident_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2611f",
   "metadata": {},
   "source": [
    "In order to label the accidents with the appropriate neighborhood, I will create a dictionary that has the neighborhood as the key and the Polygon shape as the value.  Then I can look at each accident, capture its coordinates and cycle through each dictionary item to identify which neighborhood the accident was located in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty Dictionary to start\n",
    "neighborhood_dict = {}\n",
    "\n",
    "for n, neighborhood in enumerate(coor_df):\n",
    "    #Robust selection of the coordinates for each neighborhood, not including (0, 0) values\n",
    "    coord_list = coor_df[neighborhood].loc[coor_df[neighborhood] != (0.0, 0.0)]\n",
    "    \n",
    "    #Make Polygon for the neighborhood\n",
    "    polygon = Polygon(coord_list)\n",
    "    \n",
    "    #Append dictionary by indexing the column names as the keys and the polygon as the values\n",
    "    neighborhood_dict[coor_df.columns[n]] = polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c603b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new column to input the neighborhood data\n",
    "#Set as N/A initially to check transformation afterwards\n",
    "accident_df['Neighborhood'] = 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344212ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling the neighborhood to the accident dataframe\n",
    "for row in range(0, len(accident_df)):\n",
    "    #Find the accident's coordinates and place them into Point\n",
    "    long = accident_df['Longitude'][row]\n",
    "    lat = accident_df['Latitude'][row]\n",
    "    point = Point(long, lat)\n",
    "    \n",
    "    #Loop through each neighborhood\n",
    "    for n in neighborhood_dict.keys():\n",
    "        #If the Point is inside the neighborhood Polygon, return True and label the row with the neighborhood\n",
    "        if neighborhood_dict[n].contains(point) == True:\n",
    "            accident_df['Neighborhood'][row] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea236118",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df['Neighborhood'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c6f29",
   "metadata": {},
   "source": [
    "I suspect that the N/A values are located in Shaughnessy and this is the result of skipping the bad lines when importing the initial data.  I will inspect this more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df[accident_df['Neighborhood'] =='N/A'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef708a",
   "metadata": {},
   "source": [
    "This is mostly accurate, but inspecting separately using Tableau there is some other areas that are mislabeled:\n",
    "\n",
    "1. The Stanley Park area is not assigned a location.  While technically this would be located in the 'West End' from an analysis point of view this does not seem appropriate as the traffic patterns between the West End and Stanley Park are very different.  Accidents that fall in this area will be labeled with Stanley Park\n",
    "\n",
    "2. There are a number of bridges in Vancouver, but only accidents that happened on the Oak Street Bridge seems to be mislabeled.  The Oak Street bridge leads into the Marpole neighborhood, so these will be labeled as such.\n",
    "\n",
    "3. A small collection of accidents along Boundary Road (which is the border between Vancouver and Burnaby) are unclassified.  Depending on the location, these either belong to the Killarney or the Renfrew-Collingwood neighborhoods.\n",
    "\n",
    "4. Another small collection that are near UBC (which is outside of the jurisdiction for the City of Vancouver) are also mislabeled.  Depending of the location, these either belong to West Point Grey or Dunbar-Southlands\n",
    "\n",
    "5. The remainder appear to be Shaughnessy.\n",
    "\n",
    "We will correct these here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default all N/A's to Shaughnessy\n",
    "accident_df.loc[(accident_df['Neighborhood'] =='N/A'), 'Neighborhood'] = 'Shaughnessy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the most northern points to reclassify as Stanley Park\n",
    "accident_df.loc[(accident_df['Neighborhood'] =='Shaughnessy') \n",
    "                & (accident_df['Latitude'] > 49.289), 'Neighborhood'] = 'Stanley Park'\n",
    "\n",
    "#Identifying OAK ST BRIDGE\n",
    "accident_df.loc[(accident_df['Neighborhood'] =='Shaughnessy') \n",
    "                & (accident_df['Road Location Description'] == 'OAK ST BRIDGE'), 'Neighborhood'] = 'Marpole'\n",
    "\n",
    "#Identifying Boundary Rd Neighborhoods\n",
    "accident_df.loc[(accident_df['Neighborhood'] =='Shaughnessy') \n",
    "                & (accident_df['Longitude'] > -123.03)\n",
    "                & (accident_df['Latitude'] >= 49.2325), 'Neighborhood'] = 'Renfrew-Collingwood'\n",
    "\n",
    "accident_df.loc[(accident_df['Neighborhood'] =='Shaughnessy') \n",
    "                & (accident_df['Longitude'] > -123.03)\n",
    "                & (accident_df['Latitude'] < 49.2325), 'Neighborhood'] = 'Killarney'\n",
    "\n",
    "#Identifying Near UBC Neighborhoods\n",
    "accident_df.loc[(accident_df['Neighborhood'] =='Shaughnessy') \n",
    "                & (accident_df['Longitude'] < -123.17)\n",
    "                & (accident_df['Latitude'] > 49.2582), 'Neighborhood'] = 'West Point Grey'\n",
    "\n",
    "accident_df.loc[(accident_df['Neighborhood'] =='Shaughnessy') \n",
    "                & (accident_df['Longitude'] < -123.17)\n",
    "                & (accident_df['Latitude'] < 49.2582), 'Neighborhood'] = 'Dunbar-Southlands'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df['Neighborhood'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f3115",
   "metadata": {},
   "source": [
    "## Preliminary Data Cleaning\n",
    "\n",
    "<a id = 'cleaning'></a>\n",
    "\n",
    "We now have each accident accurately labeled with their neighborhood.  With this we are now going to remove redundent columns in preparation for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b20e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df[accident_df['Municipality Name'] != accident_df['Crash Breakdown 2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df[accident_df['Region'] != 'LOWER MAINLAND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476918d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df[accident_df['Street Full Name (ifnull)'] != accident_df['Street Full Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df[accident_df['Municipality Name (ifnull)'] != accident_df['Municipality Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71883186",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df[accident_df['Metric Selector'] != accident_df['Total Crashes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df[accident_df['Road Location Description'] != accident_df['Street Full Name']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5435a",
   "metadata": {},
   "source": [
    "Considering that all of the data is known to be in Vancouver, keeping any label for the municipality name is not necessary, as well as the `Region` name (Lower Mainland).\n",
    "\n",
    "The `Road Location Description` appears to be a concatenation of the `Street Name` and the `Cross Street Name`.  Considering there is occasionally more than one cross street listed, these can be kept in their entirety for the time being.\n",
    "\n",
    "`Metric Selector` seems to duplicate `Total Crashes`, so I will just keep total crashes.]\n",
    "\n",
    "`Street Full Name (ifnull)` is a duplicate of `Street Full Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d20570",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Region', 'Crash Breakdown 2', 'Municipality Name (ifnull)', 'Street Full Name (ifnull)',\n",
    "            'Municipality Name', 'Metric Selector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df = accident_df.drop(columns = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd35f9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With this, we have appropriately added the neighborhood names and done some preliminary cleaning of the data.  This data will now be written to a csv so that it can then be input into SQL.\n",
    "\n",
    "A SQL script will then be used in order to place this data into three separate tables:\n",
    "1. Time Data (Month, Year, etc)\n",
    "2. Location Data (Neighborhood, Street Name, etc)\n",
    "3. Tag Data (Animal Flag, Cyclist Flag, etc)\n",
    "\n",
    "From there, a more thorough analysis will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbea004",
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_df.to_csv('data/vancouver_accidents.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
